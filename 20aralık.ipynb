{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOK+chYox5D79Glw5OjCK2o",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/serrabaysal/auto-ml-course-notes/blob/main/20aral%C4%B1k.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#  Makine Öğrenmesinin Sınırları\n",
        "1. Meta yaklaşım\n",
        "*   Görevler arası bilgi aktarımı sağlar\n",
        "*   Sınırlı veri\n",
        "*   öğrenme algoritmasını günceller.\n",
        "*   iç ve dış döngüler\n",
        "klasik makine öğrenmesi\n",
        "2. Metrik tabanlı ağaçlar\n",
        "*   siyam ağaçları\n",
        "*   matching networks\n",
        "*   prototypical networks:veriyi gömülü bir şekilde\n",
        "3. MAML Yaklaşımları\n",
        "Model Agnostic Meta Learning\n",
        "4. Gelişmiş Optimizasyonlar\n",
        "5. Model Tabanlı Yaklaşımlar\n",
        "* MANN:\n",
        "* HyperNetworks\n",
        "* SNAIL\n",
        "6. AUTO-ML\n",
        "7. Modern Uygulamalar\n",
        "*  Robotik ve Meta-Öğrenme\n",
        "8. AUTOML'de META YAKLAŞIM\n",
        "9. Metrik Tabanlı Yaklaşım\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "dvdSKIbRhPS7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Reptile\n",
        "* Protonet\n",
        "* Auto-Ml\n",
        "1. Few Shot:\n",
        "2. Hibrit dönem"
      ],
      "metadata": {
        "id": "gg3TuWAfrqT5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# RL TABANLI AUTOML YAKLAŞIMLAR\n",
        "1. Reinforcement Learning'e yaklaşımlar:karar verici ajan. ödül veya ceza yöntemi en yüksek kümeleme süreciyle alıyor.Agentlar çevreden bilgi alıyor.\n",
        "Oyunlarda,coğrafi bilgi sisteminde,kendi kendine yürümeyi öğrenen robotlar.\n",
        "Hedefimi maksimize etmek.\n",
        "   Markov Karar Süreçleri: MDP formatında RL problemleri ifade edilmesi gerekir. Gelecek durum, sadece şimdiki duruma bağlıdır.geçmişin bir önemi yoktur.\n",
        "*   Sömürü:Mevcut bilinen en iyi çözümün etrafında küçük değişiklikler yapmak.\n",
        "*   Keşif:Rasgele ve belirsizliği yüksek bölgelerde arama yapmak.\n",
        "*  Policy Gradient Yöntemleri\n",
        "2. Neden AUTOML için RL\n",
        "*  Sıralı kararlar\n",
        "*  Black-Box Optimizasyon\n",
        "*  Uzun vadeli planlama\n",
        "3. RL İle hiperparametre optimizasyonu\n",
        "*  HBO problemi ve RL eşleşmesi\n",
        "*  validation accuracy veya loss kayıp hata fonksiyonunu minimize etmek\n",
        "*  sürekli :batch size [16,32,64,128]\n",
        "*  optimizer ,:adam ..\n",
        "4. Google Visier:\n",
        "*  Random Search, Bayes, RL\n",
        "*  Random search her seferinde 0'dan başlar\n",
        "*  RL normal şartlardan daha fazla veri olması gerekiyor.\n",
        "*  Özellik seçimi problemi RL formülasyonu: durum,eylem,ödül\n",
        "*  RL daha esnek featurın işe yaramdağını gösterir  veya ödül kısmına geçebiliriz.flipler mesela 0 ve 1 gibi\n",
        "#  NAS:RL LEARNİNG içerisinde var.\n",
        "1. LSTM ağı kurmayı sağlıyordu.\n",
        "2. Kontroller mantığı RNN güncellemek için kullanılır.LSTM öğretmen rolünde\n",
        "öğrenci olan çocuk ağıdı.\n",
        "3. Blok Tabanlı Arama\n",
        "4. Her çocuk ağını sıfırdan eğitir. ENAS daha hızlı NAS'a göre.\n"
      ],
      "metadata": {
        "id": "NnsQ98iAyw1Y"
      }
    }
  ]
}