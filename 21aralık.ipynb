{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iTEdWAsYq8lY"
      },
      "source": [
        "# Transfer Learning\n",
        "1. Her veri için tekrardan eğitilmemize gerek kalmadan kendi problemimize uyarlamış oluyoruz.\n",
        "2. Verilerin bulunduğu ham vektör,bir örneğe ait feature var.Hedef alanı verisi az kaynak verisi de fazladır.\n",
        "3. Transfer learning yaparsam accuracy daha yüksek olarak başlar.zamandan da kazanç olmuş olur.\n",
        "4. Negatif transfer riski: transfer automl modellerini kullanarak benzerlik metriklerini yoksa transfer işlemi zararlı hale gelmiş oluyor.\n",
        "5. Kaynak ve hedef veri farklı olabilir.İstatiksel benzerlikler önemli\n",
        "6. Transductive (Dönüştürücü) Transfer\n",
        "7. Denetimsiz Transfer:\n",
        "8. Fine-tuning önceden eğitilmiş üretilen modeli hafif bir şekilde değiştirmek. son katman atılır. yeni sınıflandırmaya uygun bir yöntem üretilir.\n",
        "9. Feature çıkartılsın. burası da sınıflandırdığı kısım. Imagenet veri setini kuruyoruz onun sistemini tab kısmı feature kısmını özelliklerini kullanıyoruz.en sonunda sınıflandırma kısmını ayırt ediyorum.modelin ilk %80 ortalama olarak kısmı doldurulur.\n",
        "10. Feature extraction(özellik çıkarımı):\n",
        "11. F1 score dengesiz veri setlerinde kullanılır.\n",
        "12. Mobilenet:keras applications , efficient bir model.Mobil cihazlarda train işlemi yapmıyoruz. Test veya gerçek uygulama şeklinde kullanıyoruz.\n",
        "13. Multitask learning(Çoklu Görev öğrenme):Çoklu görev öğrenimi sesten görüntüye,metinden görüntüye,\n",
        "14. NAS sırasında her aday tekrardan eğitilmek yerine supernet eğitiyorum sıfırdan tekrardan aramak varken\n",
        "13. Eğitilmiş güzel modeller hangi modelin birbirini tamamlaması\n",
        "14. AUTOML model sayısı belli bu yüzden sürekli öğrenme var.\n",
        "15. küçük veri seti çıkmazı\n",
        "16. Few shot: benzerlik sağlayıp önceden öğretilmiş modelin uzayını kullanarak model oluşturmayı da sağlıyor.\n",
        "17. Prototip ağlar: her sınıfın ortalama vektörü açıklanıyordu. daha sonra benzerlikle bir sınıfa dahil edtiliyordu.\n",
        "18. Zeo shot:Hiçbir eğitim görmeden direkt sınıflandırma: veri azsa\n",
        "19. Foundation Modeller: (Temel Modeller) metanın nlp modelleri\n",
        "20. Sentetik veri ve transfer : çok iyi eğitilmişş gerekiyor. ana modele transfer edilebilir.\n",
        "21. Data Augmentaion: model işleme anında yapılır. sadece trainde yapılır. validation ve test de yapılmaz diye kaynaklarda geçebilir. Fakat 2024-2025 arasında yeni çıkan kaynaklarda test aşamasında TTE yaklaşımıyla yapılabiliyor.\n",
        "22. MAML (Model Agnostic ): aşırı hızlı öğrenme\n",
        "23. LLM-: gelecekte veriler llm aşamasıyla eğitilecek. ve normal model\n",
        "transformer modeller ve attention mekanizması işin içerisine girdiğinde daha fazla artıyor.\n",
        "24. Transfer learning en iyi parametrelerle en iyi veriyi buluyor.\n",
        "H2O:java tabanlı kurumsal firmalar\n",
        "25. sonra data augmentation inputla veriyi al"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "VIpqFHpfq3ty",
        "outputId": "6dc6710d-25ba-4ee4-f9cd-75a812cf79cb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.4/42.4 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.6/43.6 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.6/43.6 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m259.5/259.5 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m515.2/515.2 kB\u001b[0m \u001b[31m17.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m227.6/227.6 kB\u001b[0m \u001b[31m15.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.9/98.9 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m74.4/74.4 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m452.1/452.1 kB\u001b[0m \u001b[31m31.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m140.6/140.6 kB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m88.5/88.5 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m827.9/827.9 kB\u001b[0m \u001b[31m40.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.5/410.5 kB\u001b[0m \u001b[31m25.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.7/52.7 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m125.9/125.9 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m75.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m963.5/963.5 kB\u001b[0m \u001b[31m42.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.6/14.6 MB\u001b[0m \u001b[31m88.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.3/72.3 MB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m102.2/102.2 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.5/49.5 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m849.5/849.5 kB\u001b[0m \u001b[31m45.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m131.9/131.9 kB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.8/2.8 MB\u001b[0m \u001b[31m89.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.2/87.2 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.0/6.0 MB\u001b[0m \u001b[31m111.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m201.3/201.3 kB\u001b[0m \u001b[31m14.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m128.2/128.2 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.3/55.3 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m90.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m469.0/469.0 kB\u001b[0m \u001b[31m28.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for nvidia-ml-py3 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for seqeval (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "rasterio 1.4.4 requires click!=8.2.*,>=4.0, but you have click 8.2.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.7/124.7 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.4/129.4 kB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m645.0/645.0 MB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow-decision-forests 1.12.0 requires tensorflow==2.19.0, but you have tensorflow 2.19.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m512.3/512.3 kB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m554.0/554.0 kB\u001b[0m \u001b[31m30.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.5/9.5 MB\u001b[0m \u001b[31m115.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.7/47.7 MB\u001b[0m \u001b[31m14.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.7/64.7 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m144.7/144.7 kB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "autogluon-common 1.5.0 requires pyarrow<21.0.0,>=7.0.0, but you have pyarrow 22.0.0 which is incompatible.\n",
            "google-colab 1.0.0 requires requests==2.32.4, but you have requests 2.32.5 which is incompatible.\n",
            "tensorflow-decision-forests 1.12.0 requires tensorflow==2.19.0, but you have tensorflow 2.19.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        },
        {
          "ename": "ImportError",
          "evalue": "cannot import name '_yeojohnson_lambda' from 'sklearn.utils.fixes' (/usr/local/lib/python3.12/dist-packages/sklearn/utils/fixes.py)",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3630324550.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m from sklearn.metrics import (\n\u001b[1;32m     65\u001b[0m     \u001b[0maccuracy_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf1_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mroc_auc_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfusion_matrix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/model_selection/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtyping\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m from ._classification_threshold import (\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0mFixedThresholdClassifier\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mTunedThresholdClassifierCV\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/model_selection/_classification_threshold.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m )\n\u001b[1;32m     16\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexceptions\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mNotFittedError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m from ..metrics import (\n\u001b[0m\u001b[1;32m     18\u001b[0m     \u001b[0mcheck_scoring\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0mget_scorer_names\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/metrics/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# SPDX-License-Identifier: BSD-3-Clause\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcluster\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m from ._classification import (\n\u001b[1;32m      8\u001b[0m     \u001b[0maccuracy_score\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/metrics/cluster/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0mv_measure_score\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m )\n\u001b[0;32m---> 28\u001b[0;31m from ._unsupervised import (\n\u001b[0m\u001b[1;32m     29\u001b[0m     \u001b[0mcalinski_harabasz_score\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0mdavies_bouldin_score\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/metrics/cluster/_unsupervised.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparse\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0missparse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m...\u001b[0m\u001b[0mpreprocessing\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLabelEncoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m...\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_safe_indexing\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheck_random_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheck_X_y\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m...\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_array_api\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_atol_for_type\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/preprocessing/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# SPDX-License-Identifier: BSD-3-Clause\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m from ._data import (\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mBinarizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mKernelCenterer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/preprocessing/_data.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_param_validation\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mInterval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mOptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mStrOptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidate_params\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextmath\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_incremental_mean_and_var\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrow_norms\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfixes\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_yeojohnson_lambda\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m from ..utils.sparsefuncs import (\n\u001b[1;32m     33\u001b[0m     \u001b[0mincr_mean_variance_axis\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mImportError\u001b[0m: cannot import name '_yeojohnson_lambda' from 'sklearn.utils.fixes' (/usr/local/lib/python3.12/dist-packages/sklearn/utils/fixes.py)",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "# =========================================\n",
        "# 0) KURULUM + GLOBAL AYARLAR\n",
        "# =========================================\n",
        "\"\"\"\n",
        "UZUN ÖZET (GLOBAL)\n",
        "------------------\n",
        "Bu notebook 4 örnekten oluşur ve her örnekte aynı problem üzerinde iki yaklaşımı karşılaştırır:\n",
        "(1) AutoML: mümkün olduğunca az kodla, model seçimi + hiperparametre arama + ensemble süreçlerini otomatikleştirir.\n",
        "(2) Transfer Learning (TL): önceden eğitilmiş (pretrained) modelleri alır, yeni probleme uyarlamak için fine-tuning uygular.\n",
        "\n",
        "Ortak gereksinimler:\n",
        "- Split stratejisi: %60 train, %20 validation, %20 test\n",
        "- Süre bütçesi: Varsayılan 5 dk; tek değişkenle artırılabilir: BUDGET_MIN\n",
        "- Her örnekte skorlar bir tabloda karşılaştırılır.\n",
        "- Öğrencilerin farklı ekosistemleri görmesi için:\n",
        "  * Keras Applications (Vision TL) -> 1 kez\n",
        "  * PyTorch Hub (Vision TL)       -> 1 kez\n",
        "  * TensorFlow Hub (NLP TL)         -> 1 kez\n",
        "  * Hugging Face (NLP TL)         -> 1 kez\n",
        "\n",
        "Not:\n",
        "- 5 dakikalık demo için veri setleri tfds üzerinden seçildi (kolay erişim, Kaggle kimlik gerektirmez).\n",
        "- İsterseniz daha sonra aynı şablonu Kaggle veri setlerine (Intel / Cassava Kaggle sürümü / TR sentiment) kolayca uyarlayabiliriz.\n",
        "\"\"\"\n",
        "\n",
        "# ====== ZAMAN BÜTÇESİ (tek değişken) ======\n",
        "BUDGET_MIN = 5              # default 5 dakika; artırmak için örn. 15 yapın\n",
        "RANDOM_SEED = 42\n",
        "\n",
        "# ====== AutoML bütçeleri ======\n",
        "AUTOML_TIME_LIMIT = int(BUDGET_MIN * 60)   # saniye\n",
        "\n",
        "# ====== TL bütçeleri (yaklaşık) ======\n",
        "# 5 dk => 3-5 epoch; 15 dk => 10-15 epoch gibi ölçekleme\n",
        "def epochs_from_budget(budget_min: int) -> int:\n",
        "    if budget_min <= 5:\n",
        "        return 4\n",
        "    if budget_min <= 10:\n",
        "        return 8\n",
        "    return 12\n",
        "\n",
        "MAX_EPOCHS = epochs_from_budget(BUDGET_MIN)\n",
        "\n",
        "# ====== Demo hızlandırma (opsiyonel subsample) ======\n",
        "# 5 dk modunda çok büyük veri gelirse kısmi örnekleme yapacağız.\n",
        "FAST_DEMO = True\n",
        "MAX_TRAIN_SAMPLES = 8000 if FAST_DEMO and BUDGET_MIN <= 5 else None\n",
        "MAX_VAL_SAMPLES   = 2000 if FAST_DEMO and BUDGET_MIN <= 5 else None\n",
        "MAX_TEST_SAMPLES  = 2000 if FAST_DEMO and BUDGET_MIN <= 5 else None\n",
        "\n",
        "# ========== Paket kurulumları ==========\n",
        "# AutoGluon (tabular & text), AutoKeras (vision AutoML), TabPFN (tabular TL),\n",
        "# TensorFlow Datasets, TF Hub, Torch, Transformers\n",
        "!pip -q install -U autogluon.tabular autogluon.multimodal\n",
        "!pip -q install -U autokeras tensorflow tensorflow-hub tensorflow-datasets\n",
        "!pip -q install -U torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\n",
        "!pip -q install -U transformers datasets evaluate scikit-learn tabpfn\n",
        "\n",
        "import os, time, math, random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score, f1_score, log_loss, roc_auc_score, confusion_matrix\n",
        ")\n",
        "\n",
        "np.random.seed(RANDOM_SEED)\n",
        "random.seed(RANDOM_SEED)\n",
        "\n",
        "def split_60_20_20(X, y, stratify=True):\n",
        "    \"\"\"\n",
        "    60/20/20 split:\n",
        "      Train = 60%\n",
        "      Val   = 20%\n",
        "      Test  = 20%\n",
        "    \"\"\"\n",
        "    strat = y if stratify else None\n",
        "    X_train, X_temp, y_train, y_temp = train_test_split(\n",
        "        X, y, test_size=0.40, random_state=RANDOM_SEED, stratify=strat\n",
        "    )\n",
        "    strat2 = y_temp if stratify else None\n",
        "    X_val, X_test, y_val, y_test = train_test_split(\n",
        "        X_temp, y_temp, test_size=0.50, random_state=RANDOM_SEED, stratify=strat2\n",
        "    )\n",
        "    return X_train, X_val, X_test, y_train, y_val, y_test\n",
        "\n",
        "def maybe_subsample(X, y, max_n):\n",
        "    if max_n is None or len(X) <= max_n:\n",
        "        return X, y\n",
        "    idx = np.random.choice(len(X), size=max_n, replace=False)\n",
        "    if isinstance(X, pd.DataFrame):\n",
        "        return X.iloc[idx].reset_index(drop=True), y.iloc[idx].reset_index(drop=True)\n",
        "    return X[idx], y[idx]\n",
        "\n",
        "def summarize_binary_metrics(y_true, proba_pos, y_pred=None):\n",
        "    if y_pred is None:\n",
        "        y_pred = (proba_pos >= 0.5).astype(int)\n",
        "    out = {}\n",
        "    out[\"Accuracy\"] = accuracy_score(y_true, y_pred)\n",
        "    out[\"MacroF1\"] = f1_score(y_true, y_pred, average=\"macro\")\n",
        "    try:\n",
        "        out[\"ROC-AUC\"] = roc_auc_score(y_true, proba_pos)\n",
        "    except Exception:\n",
        "        out[\"ROC-AUC\"] = np.nan\n",
        "    try:\n",
        "        out[\"LogLoss\"] = log_loss(y_true, np.c_[1-proba_pos, proba_pos], labels=[0,1])\n",
        "    except Exception:\n",
        "        out[\"LogLoss\"] = np.nan\n",
        "    return out\n",
        "\n",
        "def summarize_multiclass_metrics(y_true, proba, y_pred=None):\n",
        "    if y_pred is None:\n",
        "        y_pred = np.argmax(proba, axis=1)\n",
        "    out = {}\n",
        "    out[\"Accuracy\"] = accuracy_score(y_true, y_pred)\n",
        "    out[\"MacroF1\"] = f1_score(y_true, y_pred, average=\"macro\")\n",
        "    try:\n",
        "        out[\"LogLoss\"] = log_loss(y_true, proba)\n",
        "    except Exception:\n",
        "        out[\"LogLoss\"] = np.nan\n",
        "    return out\n",
        "\n",
        "print(\"BUDGET_MIN:\", BUDGET_MIN, \"AUTOML_TIME_LIMIT(s):\", AUTOML_TIME_LIMIT, \"MAX_EPOCHS:\", MAX_EPOCHS)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "9FbZI6wgOZsv",
        "outputId": "1cbaf9d5-bca2-479d-a622-d3eecab650f5"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "No path specified. Models will be saved in: \"AutogluonModels/ag-20251221_092851\"\n",
            "Verbosity: 2 (Standard Logging)\n",
            "=================== System Info ===================\n",
            "AutoGluon Version:  1.5.0\n",
            "Python Version:     3.12.12\n",
            "Operating System:   Linux\n",
            "Platform Machine:   x86_64\n",
            "Platform Version:   #1 SMP Thu Oct  2 10:42:05 UTC 2025\n",
            "CPU Count:          2\n",
            "Pytorch Version:    2.9.0+cpu\n",
            "CUDA Version:       CUDA is not available\n",
            "Memory Avail:       11.04 GB / 12.67 GB (87.2%)\n",
            "Disk Space Avail:   83.51 GB / 107.72 GB (77.5%)\n",
            "===================================================\n",
            "Presets specified: ['medium_quality']\n",
            "Using hyperparameters preset: hyperparameters='default'\n",
            "Beginning AutoGluon training ... Time limit = 300s\n",
            "AutoGluon will save models to \"/content/AutogluonModels/ag-20251221_092851\"\n",
            "Train Data Rows:    534\n",
            "Train Data Columns: 11\n",
            "Tuning Data Rows:    178\n",
            "Tuning Data Columns: 11\n",
            "Label Column:       survived\n",
            "AutoGluon infers your prediction problem is: 'binary' (because only two unique label-values observed).\n",
            "\t2 unique label values:  [np.int64(0), np.int64(1)]\n",
            "\tIf 'binary' is not the correct problem_type, please manually specify the problem_type parameter during Predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression', 'quantile'])\n",
            "Problem Type:       binary\n",
            "Preprocessing data ...\n",
            "Selected class <--> label mapping:  class 1 = 1, class 0 = 0\n",
            "Using Feature Generators to preprocess the data ...\n",
            "Fitting AutoMLPipelineFeatureGenerator...\n",
            "\tAvailable Memory:                    11311.45 MB\n",
            "\tTrain Data (Original)  Memory Usage: 0.14 MB (0.0% of available memory)\n",
            "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
            "\tStage 1 Generators:\n",
            "\t\tFitting AsTypeFeatureGenerator...\n",
            "\t\t\tNote: Converting 3 features to boolean dtype as they only contain 2 unique values.\n",
            "\tStage 2 Generators:\n",
            "\t\tFitting FillNaFeatureGenerator...\n",
            "\tStage 3 Generators:\n",
            "\t\tFitting IdentityFeatureGenerator...\n",
            "\t\tFitting CategoryFeatureGenerator...\n",
            "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
            "\tStage 4 Generators:\n",
            "\t\tFitting DropUniqueFeatureGenerator...\n",
            "\tStage 5 Generators:\n",
            "\t\tFitting DropDuplicatesFeatureGenerator...\n",
            "\tTypes of features in original data (raw dtype, special dtypes):\n",
            "\t\t('bool', [])     : 2 | ['adult_male', 'alone']\n",
            "\t\t('category', []) : 1 | ['class']\n",
            "\t\t('float', [])    : 2 | ['age', 'fare']\n",
            "\t\t('int', [])      : 3 | ['pclass', 'sibsp', 'parch']\n",
            "\t\t('object', [])   : 3 | ['sex', 'embarked', 'who']\n",
            "\tTypes of features in processed data (raw dtype, special dtypes):\n",
            "\t\t('category', [])  : 3 | ['embarked', 'class', 'who']\n",
            "\t\t('float', [])     : 2 | ['age', 'fare']\n",
            "\t\t('int', [])       : 3 | ['pclass', 'sibsp', 'parch']\n",
            "\t\t('int', ['bool']) : 3 | ['sex', 'adult_male', 'alone']\n",
            "\t0.1s = Fit runtime\n",
            "\t11 features in original data used to generate 11 features in processed data.\n",
            "\tTrain Data (Processed) Memory Usage: 0.03 MB (0.0% of available memory)\n",
            "Data preprocessing and feature engineering runtime = 0.15s ...\n",
            "AutoGluon will gauge predictive performance using evaluation metric: 'log_loss'\n",
            "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
            "\tThis metric expects predicted probabilities rather than predicted class labels, so you'll need to use predict_proba() instead of predict()\n",
            "\tTo change this, specify the eval_metric parameter of Predictor()\n",
            "User-specified model hyperparameters to be fit:\n",
            "{\n",
            "\t'NN_TORCH': [{}],\n",
            "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, {'learning_rate': 0.03, 'num_leaves': 128, 'feature_fraction': 0.9, 'min_data_in_leaf': 3, 'ag_args': {'name_suffix': 'Large', 'priority': 0, 'hyperparameter_tune_kwargs': None}}],\n",
            "\t'CAT': [{}],\n",
            "\t'XGB': [{}],\n",
            "\t'FASTAI': [{}],\n",
            "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "}\n",
            "Fitting 11 L1 models, fit_strategy=\"sequential\" ...\n",
            "Fitting model: LightGBMXT ... Training model for up to 299.85s of the 299.85s of remaining time.\n",
            "\tFitting with cpus=1, gpus=0, mem=0.0/11.0 GB\n",
            "\t-0.3854\t = Validation score   (-log_loss)\n",
            "\t3.48s\t = Training   runtime\n",
            "\t0.01s\t = Validation runtime\n",
            "Fitting model: LightGBM ... Training model for up to 296.35s of the 296.35s of remaining time.\n",
            "\tFitting with cpus=1, gpus=0, mem=0.0/11.0 GB\n",
            "\t-0.3866\t = Validation score   (-log_loss)\n",
            "\t0.33s\t = Training   runtime\n",
            "\t0.0s\t = Validation runtime\n",
            "Fitting model: RandomForestGini ... Training model for up to 296.01s of the 296.00s of remaining time.\n",
            "\tFitting with cpus=2, gpus=0\n",
            "\t-0.6245\t = Validation score   (-log_loss)\n",
            "\t0.83s\t = Training   runtime\n",
            "\t0.08s\t = Validation runtime\n",
            "Fitting model: RandomForestEntr ... Training model for up to 295.07s of the 295.07s of remaining time.\n",
            "\tFitting with cpus=2, gpus=0\n",
            "\t-0.6114\t = Validation score   (-log_loss)\n",
            "\t0.84s\t = Training   runtime\n",
            "\t0.09s\t = Validation runtime\n",
            "Fitting model: CatBoost ... Training model for up to 294.13s of the 294.12s of remaining time.\n",
            "\tFitting with cpus=1, gpus=0\n",
            "\tWarning: Exception caused CatBoost to fail during training (ImportError)... Skipping this model.\n",
            "\t\t`import catboost` failed. A quick tip is to install via `pip install autogluon.tabular[catboost]==1.5.0`.\n",
            "Fitting model: ExtraTreesGini ... Training model for up to 293.90s of the 293.90s of remaining time.\n",
            "\tFitting with cpus=2, gpus=0\n",
            "\t-0.5959\t = Validation score   (-log_loss)\n",
            "\t0.83s\t = Training   runtime\n",
            "\t0.09s\t = Validation runtime\n",
            "Fitting model: ExtraTreesEntr ... Training model for up to 292.95s of the 292.95s of remaining time.\n",
            "\tFitting with cpus=2, gpus=0\n",
            "\t-0.6614\t = Validation score   (-log_loss)\n",
            "\t0.8s\t = Training   runtime\n",
            "\t0.08s\t = Validation runtime\n",
            "Fitting model: NeuralNetFastAI ... Training model for up to 292.04s of the 292.04s of remaining time.\n",
            "\tFitting with cpus=1, gpus=0, mem=0.0/11.0 GB\n",
            "No improvement since epoch 9: early stopping\n",
            "\t-0.4256\t = Validation score   (-log_loss)\n",
            "\t3.0s\t = Training   runtime\n",
            "\t0.03s\t = Validation runtime\n",
            "Fitting model: XGBoost ... Training model for up to 288.99s of the 288.99s of remaining time.\n",
            "\tFitting with cpus=1, gpus=0\n",
            "\t-0.3866\t = Validation score   (-log_loss)\n",
            "\t0.69s\t = Training   runtime\n",
            "\t0.01s\t = Validation runtime\n",
            "Fitting model: NeuralNetTorch ... Training model for up to 288.27s of the 288.27s of remaining time.\n",
            "\tFitting with cpus=1, gpus=0, mem=0.0/11.0 GB\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/compose/_column_transformer.py:975: FutureWarning: The parameter `force_int_remainder_cols` is deprecated and will be removed in 1.9. It has no effect. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "\t-0.3929\t = Validation score   (-log_loss)\n",
            "\t8.39s\t = Training   runtime\n",
            "\t0.02s\t = Validation runtime\n",
            "Fitting model: LightGBMLarge ... Training model for up to 279.86s of the 279.86s of remaining time.\n",
            "\tFitting with cpus=1, gpus=0, mem=0.1/10.9 GB\n",
            "\t-0.4143\t = Validation score   (-log_loss)\n",
            "\t0.62s\t = Training   runtime\n",
            "\t0.0s\t = Validation runtime\n",
            "Fitting model: WeightedEnsemble_L2 ... Training model for up to 299.85s of the 279.20s of remaining time.\n",
            "\tFitting 1 model on all data | Fitting with cpus=2, gpus=0, mem=0.0/10.9 GB\n",
            "\tEnsemble Weights: {'XGBoost': 0.375, 'NeuralNetTorch': 0.375, 'LightGBMXT': 0.167, 'RandomForestEntr': 0.083}\n",
            "\t-0.3751\t = Validation score   (-log_loss)\n",
            "\t0.02s\t = Training   runtime\n",
            "\t0.0s\t = Validation runtime\n",
            "AutoGluon training complete, total runtime = 20.84s ... Best model: WeightedEnsemble_L2 | Estimated inference throughput: 1467.2 rows/s (178 batch size)\n",
            "Disabling calibration for metric `log_loss` due to having fewer than 3000 rows of validation data for calibration, to avoid overfitting (178 rows). Force calibration via specifying `calibrate=True`. (calibrate='auto')\n",
            "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"/content/AutogluonModels/ag-20251221_092851\")\n"
          ]
        },
        {
          "ename": "TypeError",
          "evalue": "TabularMLP.__init__() takes 1 positional argument but 2 were given",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3485708266.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    127\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 129\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTabularMLP\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXtr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    130\u001b[0m \u001b[0mcriterion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCrossEntropyLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    489\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    490\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_super_init\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mFalse\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 491\u001b[0;31m             raise TypeError(\n\u001b[0m\u001b[1;32m    492\u001b[0m                 \u001b[0;34mf\"{type(self).__name__}.__init__() takes 1 positional argument but {len(args) + 1} were\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    493\u001b[0m                 \u001b[0;34m\" given\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: TabularMLP.__init__() takes 1 positional argument but 2 were given"
          ]
        }
      ],
      "source": [
        "# =========================================\n",
        "# 1) TABULAR (ÖĞRETİCİ) - TITANIC\n",
        "# =========================================\n",
        "\"\"\"\n",
        "UZUN ÖZET (Örnek-1: Tabular / Titanic)\n",
        "-------------------------------------\n",
        "Amaç:\n",
        "- Tabular veride AutoML (AutoGluon Tabular) ile TL yaklaşımını (TabPFN) aynı split üzerinde karşılaştırmak.\n",
        "- Öğrenciler \"tabular\" veri kavramını, leakage riskini, metrikleri ve otomasyonun rolünü görsün.\n",
        "\n",
        "Yöntemler:\n",
        "A) AutoML: AutoGluon Tabular\n",
        "   - model seçimi + HPO + ensemble/stacking\n",
        "   - time_limit ile 5 dk default.\n",
        "\n",
        "B) Transfer Learning (Tabular): TabPFN\n",
        "   - Tabular için ön-eğitimli (pretrained) bir model yaklaşımı.\n",
        "   - Küçük/orta tabular veri setlerinde güçlü baseline verir.\n",
        "   - Bu örnekte TL'nin tabular dünyadaki yerini görselleştirir.\n",
        "\n",
        "Split:\n",
        "- %60 train / %20 val / %20 test (stratified)\n",
        "\n",
        "Metrikler:\n",
        "- Accuracy, Macro-F1, ROC-AUC, LogLoss\n",
        "\n",
        "Beklenen gözlem:\n",
        "- Titanic gibi küçük tabularda AutoML ve TabPFN benzer seviyeye gelebilir;\n",
        "  AutoML'in avantajı: rapor/leaderboard ve pipeline otomasyonu.\n",
        "\"\"\"\n",
        "\n",
        "import seaborn as sns\n",
        "from autogluon.tabular import TabularPredictor\n",
        "from tabpfn import TabPFNClassifier\n",
        "\n",
        "df = sns.load_dataset(\"titanic\").dropna(subset=[\"survived\"]).copy()\n",
        "# Basit bir feature set (öğretici olması için)\n",
        "use_cols = [\n",
        "    \"survived\", \"pclass\", \"sex\", \"age\", \"sibsp\", \"parch\", \"fare\", \"embarked\", \"class\", \"who\", \"adult_male\", \"alone\"\n",
        "]\n",
        "df = df[use_cols].copy()\n",
        "\n",
        "target = \"survived\"\n",
        "X = df.drop(columns=[target])\n",
        "y = df[target].astype(int)\n",
        "\n",
        "X_train, X_val, X_test, y_train, y_val, y_test = split_60_20_20(X, y, stratify=True)\n",
        "\n",
        "# Demo subsample\n",
        "X_train, y_train = maybe_subsample(X_train, y_train, MAX_TRAIN_SAMPLES)\n",
        "X_val, y_val     = maybe_subsample(X_val, y_val, MAX_VAL_SAMPLES)\n",
        "X_test, y_test   = maybe_subsample(X_test, y_test, MAX_TEST_SAMPLES)\n",
        "\n",
        "results_1 = []\n",
        "\n",
        "# ---- A) AutoML: AutoGluon Tabular ----\n",
        "train_ag = X_train.copy()\n",
        "train_ag[target] = y_train.values\n",
        "val_ag = X_val.copy()\n",
        "val_ag[target] = y_val.values\n",
        "\n",
        "t0 = time.time()\n",
        "predictor = TabularPredictor(label=target, eval_metric=\"log_loss\").fit(\n",
        "    train_data=train_ag,\n",
        "    tuning_data=val_ag,\n",
        "    time_limit=AUTOML_TIME_LIMIT,\n",
        "    presets=\"medium_quality\"\n",
        ")\n",
        "t_ag = (time.time() - t0) / 60.0\n",
        "\n",
        "proba_test = predictor.predict_proba(X_test)  # dataframe with columns [0,1]\n",
        "proba_pos = proba_test[1].values if 1 in proba_test.columns else proba_test.iloc[:, 1].values\n",
        "m_ag = summarize_binary_metrics(y_test.values, proba_pos)\n",
        "\n",
        "results_1.append({\n",
        "    \"Example\": \"1-Tabular\",\n",
        "    \"Method\": \"AutoML\",\n",
        "    \"Model\": \"AutoGluon Tabular\",\n",
        "    **m_ag,\n",
        "    \"Train_Min\": round(t_ag, 2)\n",
        "})\n",
        "\n",
        "# ---- B) TL: TabPFN ----\n",
        "# TabPFN numeric input ister; kategorikleri one-hot ile dönüştürelim (öğretici)\n",
        "# ---- B) \"Transfer Learning\" (Tabular için gated olmayan alternatif) ----\n",
        "# Burada tabular için küçük bir derin model kuruyoruz ve \"fine-tune reçetesi\" gösteriyoruz.\n",
        "# Amaç: Öğrencilerin tabularda da representation learning + fine-tuning mantığını görmesi.\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# 1) One-hot + ölçekleme\n",
        "X_train_oh = pd.get_dummies(X_train, dummy_na=True)\n",
        "X_val_oh   = pd.get_dummies(X_val, dummy_na=True)\n",
        "X_test_oh  = pd.get_dummies(X_test, dummy_na=True)\n",
        "\n",
        "X_val_oh  = X_val_oh.reindex(columns=X_train_oh.columns, fill_value=0)\n",
        "X_test_oh = X_test_oh.reindex(columns=X_train_oh.columns, fill_value=0)\n",
        "\n",
        "scaler = StandardScaler(with_mean=True, with_std=True)\n",
        "Xtr = scaler.fit_transform(X_train_oh.values).astype(np.float32)\n",
        "Xva = scaler.transform(X_val_oh.values).astype(np.float32)\n",
        "Xte = scaler.transform(X_test_oh.values).astype(np.float32)\n",
        "\n",
        "ytr = y_train.values.astype(np.int64)\n",
        "yva = y_val.values.astype(np.int64)\n",
        "yte = y_test.values.astype(np.int64)\n",
        "\n",
        "# 2) Mini tabular model (MLP) - hızlı demo\n",
        "class TabularMLP(nn.Module):\n",
        "    def __init__(self, d_in: int):\n",
        "        super(TabularMLP, self).__init__() # Changed this line\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(d_in, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.2),\n",
        "            nn.Linear(256, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.1),\n",
        "            nn.Linear(64, 2)\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "model = TabularMLP(Xtr.shape[1]).to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# 3) Eğitim yardımcıları\n",
        "def batch_iter(X, y, bs=256, shuffle=True):\n",
        "    idx = np.arange(len(X))\n",
        "    if shuffle:\n",
        "        np.random.shuffle(idx)\n",
        "    for i in range(0, len(X), bs):\n",
        "        j = idx[i:i+bs]\n",
        "        yield X[j], y[j]\n",
        "\n",
        "def eval_proba(model, X):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        logits = model(torch.tensor(X, device=device))\n",
        "        proba = torch.softmax(logits, dim=1).cpu().numpy()\n",
        "    return proba\n",
        "\n",
        "# 4) Strateji-1: Hızlı eğitim (head gibi düşünün) - yüksek LR, az epoch\n",
        "t0 = time.time()\n",
        "opt = optim.Adam(model.parameters(), lr=1e-3)\n",
        "ep1 = max(2, min(5, MAX_EPOCHS))\n",
        "best_val = -1\n",
        "best_state = None\n",
        "\n",
        "for epoch in range(ep1):\n",
        "    model.train()\n",
        "    for xb, yb in batch_iter(Xtr, ytr, bs=256, shuffle=True):\n",
        "        xb_t = torch.tensor(xb, device=device)\n",
        "        yb_t = torch.tensor(yb, device=device)\n",
        "        opt.zero_grad()\n",
        "        loss = criterion(model(xb_t), yb_t)\n",
        "        loss.backward()\n",
        "        opt.step()\n",
        "\n",
        "    # val macro-f1 ile early-stop fikri\n",
        "    val_proba = eval_proba(model, Xva)\n",
        "    val_pred = val_proba.argmax(axis=1)\n",
        "    val_f1 = f1_score(yva, val_pred, average=\"macro\")\n",
        "    if val_f1 > best_val:\n",
        "        best_val = val_f1\n",
        "        best_state = {k: v.detach().cpu().clone() for k,v in model.state_dict().items()}\n",
        "\n",
        "t_stage1 = (time.time() - t0) / 60.0\n",
        "\n",
        "# 5) Strateji-2: Fine-tuning - düşük LR ile kısa devam\n",
        "if best_state is not None:\n",
        "    model.load_state_dict(best_state)\n",
        "\n",
        "t0 = time.time()\n",
        "opt = optim.Adam(model.parameters(), lr=1e-4)  # düşük LR\n",
        "ep2 = max(1, min(3, MAX_EPOCHS-1))\n",
        "\n",
        "for epoch in range(ep2):\n",
        "    model.train()\n",
        "    for xb, yb in batch_iter(Xtr, ytr, bs=256, shuffle=True):\n",
        "        xb_t = torch.tensor(xb, device=device)\n",
        "        yb_t = torch.tensor(yb, device=device)\n",
        "        opt.zero_grad()\n",
        "        loss = criterion(model(xb_t), yb_t)\n",
        "        loss.backward()\n",
        "        opt.step()\n",
        "\n",
        "t_stage2 = (time.time() - t0) / 60.0\n",
        "\n",
        "# 6) Test metrikleri\n",
        "proba = eval_proba(model, Xte)\n",
        "proba_pos = proba[:, 1]\n",
        "m_tl = summarize_binary_metrics(yte, proba_pos)\n",
        "\n",
        "results_1.append({\n",
        "    \"Example\": \"1-Tabular\",\n",
        "    \"Method\": \"Transfer\",\n",
        "    \"Model\": f\"TabularMLP (Stage1 ep={ep1}, Stage2 ep={ep2})\",\n",
        "    **m_tl,\n",
        "    \"Train_Min\": round(t_stage1 + t_stage2, 2)\n",
        "})\n",
        "\n",
        "print(\"\\n=== [AutoGluon] Leaderboard (Val) ===\")\n",
        "lb = predictor.leaderboard(silent=True)\n",
        "display(lb.head(15))\n",
        "\n",
        "best_model = predictor.get_model_best()\n",
        "print(\"\\n=== [AutoGluon] Best Model ===\")\n",
        "print(\"Best model name:\", best_model)\n",
        "\n",
        "print(\"\\n=== [AutoGluon] Best Model Hyperparameters ===\")\n",
        "# model hyperparameters\n",
        "try:\n",
        "    params = predictor.info()[\"model_info\"][best_model][\"hyperparameters\"]\n",
        "    print(params)\n",
        "except Exception as e:\n",
        "    print(\"Hyperparameter extraction failed:\", e)\n",
        "\n",
        "print(\"\\n=== [AutoGluon] Best Model Training Info ===\")\n",
        "info = predictor.info()\n",
        "try:\n",
        "    model_info = info[\"model_info\"][best_model]\n",
        "    for k in [\"model_type\", \"fit_time\", \"predict_time\", \"val_score\"]:\n",
        "        if k in model_info:\n",
        "            print(f\"{k}: {model_info[k]}\")\n",
        "except Exception as e:\n",
        "    print(\"Model info extraction failed:\", e)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 383
        },
        "id": "CpGb1En_jrny",
        "outputId": "76e73d0c-bbae-4dd5-9df1-c9948cb80804"
      },
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'autokeras'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3499365970.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow_datasets\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtfds\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mautokeras\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'autokeras'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "# =========================================\n",
        "# 2) VISION-1 (GERÇEK DÜNYA) - TF FLOWERS (DÜZELTİLMİŞ)\n",
        "# =========================================\n",
        "\"\"\"\n",
        "UZUN ÖZET (Örnek-2: Vision / TF Flowers) - DÜZELTİLMİŞ SÜRÜM\n",
        "------------------------------------------------------------\n",
        "Bu örnekte AutoML (AutoKeras) ve Transfer Learning (Keras Applications) aynı veri üzerinde\n",
        "karşılaştırılır. Split stratejisi %60/%20/%20 olarak uygulanır.\n",
        "\n",
        "Neden düzeltme gerekti?\n",
        "- TFDS 'tf_flowers' datasetindeki görüntüler farklı boyutlarda gelir (ör. 320x240, 500x333 vb.).\n",
        "- Bu yüzden Python listesi -> np.array dönüşümünde NumPy tek bir 4D tensör yapamaz:\n",
        "  ValueError: inhomogeneous shape.\n",
        "Çözüm:\n",
        "- Veriyi önce NumPy'ye çevirmek yerine doğrudan tf.data pipeline üzerinde resimleri resize ederek\n",
        "  tek tip boyuta getirmek (IMG_SIZE x IMG_SIZE). Böylece hem hata çözülür hem de daha verimli olur.\n",
        "\n",
        "Kullanılan yöntemler:\n",
        "A) AutoML: AutoKeras ImageClassifier\n",
        "B) Transfer Learning: Keras Applications (EfficientNetB0, ImageNet pretrained)\n",
        "   - Strateji-1: Backbone freeze + head train\n",
        "   - Strateji-2: Fine-tune (unfreeze) + düşük LR\n",
        "\n",
        "Metrikler:\n",
        "- Accuracy, Macro-F1, LogLoss + süre\n",
        "\n",
        "Bütçe:\n",
        "- BUDGET_MIN ile kontrol edilir. 5 dk varsayılan.\n",
        "\"\"\"\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "import autokeras as ak\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import time\n",
        "from sklearn.metrics import accuracy_score, f1_score, log_loss\n",
        "\n",
        "tf.random.set_seed(RANDOM_SEED)\n",
        "\n",
        "# -----------------------------\n",
        "# 1) Dataset load (tfds)\n",
        "# -----------------------------\n",
        "(ds_all, info) = tfds.load(\"tf_flowers\", with_info=True, as_supervised=True)\n",
        "ds_all = ds_all[\"train\"]\n",
        "\n",
        "N = info.splits[\"train\"].num_examples  # tf_flowers ~ 3670\n",
        "\n",
        "# -----------------------------\n",
        "# 2) %60/%20/%20 split index ile\n",
        "# -----------------------------\n",
        "n_train = int(0.60 * N)\n",
        "n_val   = int(0.20 * N)\n",
        "n_test  = N - n_train - n_val\n",
        "\n",
        "# TFDS sıralı geliyor; stratify yok. Eğitim amaçlı kabul edilebilir.\n",
        "# İsterseniz ileride stratified split için label'ları önce okuyup indeks seçeriz.\n",
        "ds_train_raw = ds_all.take(n_train)\n",
        "ds_val_raw   = ds_all.skip(n_train).take(n_val)\n",
        "ds_test_raw  = ds_all.skip(n_train + n_val).take(n_test)\n",
        "\n",
        "# -----------------------------\n",
        "# 3) Preprocess: resize + normalize\n",
        "# -----------------------------\n",
        "IMG_SIZE = 160\n",
        "NUM_CLASSES = info.features[\"label\"].num_classes\n",
        "\n",
        "def preprocess(img, label):\n",
        "    img = tf.image.resize(img, (IMG_SIZE, IMG_SIZE))\n",
        "    img = tf.cast(img, tf.float32) / 255.0\n",
        "    return img, label\n",
        "\n",
        "BATCH = 32\n",
        "\n",
        "train_ds = ds_train_raw.map(preprocess, num_parallel_calls=tf.data.AUTOTUNE)\\\n",
        "                      .shuffle(1024, seed=RANDOM_SEED)\\\n",
        "                      .batch(BATCH).prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "val_ds   = ds_val_raw.map(preprocess, num_parallel_calls=tf.data.AUTOTUNE)\\\n",
        "                    .batch(BATCH).prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "test_ds  = ds_test_raw.map(preprocess, num_parallel_calls=tf.data.AUTOTUNE)\\\n",
        "                     .batch(BATCH).prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "# AutoKeras bazen numpy ister; tf.data ile de çalışabilir ancak bazı sürümlerde sorun çıkabiliyor.\n",
        "# Bu yüzden \"tek tip boyuta gelmiş\" veri üzerinden kontrollü şekilde numpy'a geçeceğiz:\n",
        "def ds_to_numpy(ds, max_batches=None):\n",
        "    X_list, y_list = [], []\n",
        "    for i, (xb, yb) in enumerate(ds):\n",
        "        if max_batches is not None and i >= max_batches:\n",
        "            break\n",
        "        X_list.append(xb.numpy())\n",
        "        y_list.append(yb.numpy())\n",
        "    X = np.concatenate(X_list, axis=0)\n",
        "    y = np.concatenate(y_list, axis=0)\n",
        "    return X, y\n",
        "\n",
        "# 5 dk demo için opsiyonel kısaltma:\n",
        "# max_batches = None -> full\n",
        "max_batches_train = 50 if (FAST_DEMO and BUDGET_MIN <= 5) else None  # ~50*32=1600 örnek\n",
        "max_batches_val   = 15 if (FAST_DEMO and BUDGET_MIN <= 5) else None\n",
        "max_batches_test  = 15 if (FAST_DEMO and BUDGET_MIN <= 5) else None\n",
        "\n",
        "X_train, y_train = ds_to_numpy(train_ds, max_batches=max_batches_train)\n",
        "X_val,   y_val   = ds_to_numpy(val_ds,   max_batches=max_batches_val)\n",
        "X_test,  y_test  = ds_to_numpy(test_ds,  max_batches=max_batches_test)\n",
        "\n",
        "# (AutoKeras için) float32 [0,1] zaten hazır.\n",
        "results_2 = []\n",
        "\n",
        "# -----------------------------\n",
        "# 4) AutoML: AutoKeras\n",
        "# -----------------------------\n",
        "max_trials = 10 if BUDGET_MIN <= 5 else 30\n",
        "ak_epochs  = max(2, min(6, MAX_EPOCHS))\n",
        "\n",
        "t0 = time.time()\n",
        "clf = ak.ImageClassifier(overwrite=True, max_trials=max_trials)\n",
        "clf.fit(\n",
        "    X_train, y_train,\n",
        "    epochs=ak_epochs,\n",
        "    validation_data=(X_val, y_val),\n",
        "    verbose=1\n",
        ")\n",
        "t_ak = (time.time() - t0) / 60.0\n",
        "\n",
        "model_ak = clf.export_model()\n",
        "logits = model_ak.predict(X_test, verbose=1)\n",
        "proba_ak = tf.nn.softmax(logits, axis=1).numpy()\n",
        "pred_ak = np.argmax(proba_ak, axis=1)\n",
        "\n",
        "m_ak = {\n",
        "    \"Accuracy\": accuracy_score(y_test, pred_ak),\n",
        "    \"MacroF1\": f1_score(y_test, pred_ak, average=\"macro\"),\n",
        "    \"LogLoss\": log_loss(y_test, proba_ak)\n",
        "}\n",
        "\n",
        "print(\"\\n=== [AutoKeras] Best Exported Model Summary ===\")\n",
        "model_ak.summary()\n",
        "\n",
        "results_2.append({\n",
        "    \"Example\": \"2-Vision\",\n",
        "    \"Method\": \"AutoML\",\n",
        "    \"Model\": f\"AutoKeras (trials={max_trials}, ep={ak_epochs})\",\n",
        "    **m_ak,\n",
        "    \"Train_Min\": round(t_ak, 2)\n",
        "})\n",
        "\n",
        "# -----------------------------\n",
        "# 5) Transfer Learning: Keras Applications (EfficientNetB0)\n",
        "# -----------------------------\n",
        "from tensorflow.keras import layers, models\n",
        "from tensorflow.keras.applications import EfficientNetB0\n",
        "\n",
        "def build_keras_app_model(trainable_backbone: bool):\n",
        "    inp = layers.Input(shape=(IMG_SIZE, IMG_SIZE, 3))\n",
        "    x = tf.keras.applications.efficientnet.preprocess_input(inp * 255.0)  # preprocess expects 0-255\n",
        "    backbone = EfficientNetB0(include_top=False, weights=\"imagenet\", input_tensor=x)\n",
        "    backbone.trainable = trainable_backbone\n",
        "    x = layers.GlobalAveragePooling2D()(backbone.output)\n",
        "    x = layers.Dropout(0.2)(x)\n",
        "    out = layers.Dense(NUM_CLASSES)(x)\n",
        "    return models.Model(inp, out)\n",
        "\n",
        "def eval_keras(model, X, y):\n",
        "    logits = model.predict(X, verbose=1)\n",
        "    proba = tf.nn.softmax(logits, axis=1).numpy()\n",
        "    pred = np.argmax(proba, axis=1)\n",
        "    return {\n",
        "        \"Accuracy\": accuracy_score(y, pred),\n",
        "        \"MacroF1\": f1_score(y, pred, average=\"macro\"),\n",
        "        \"LogLoss\": log_loss(y, proba)\n",
        "    }\n",
        "\n",
        "# Strateji-1: Freeze\n",
        "tl_ep1 = max(2, min(4, MAX_EPOCHS))\n",
        "t0 = time.time()\n",
        "model1 = build_keras_app_model(trainable_backbone=False)\n",
        "model1.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),\n",
        "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "    metrics=[\"accuracy\"]\n",
        ")\n",
        "model1.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=tl_ep1, batch_size=32, verbose=1)\n",
        "t_freeze = (time.time() - t0) / 60.0\n",
        "\n",
        "m_tl1 = eval_keras(model1, X_test, y_test)\n",
        "\n",
        "# Model card (reçete)\n",
        "print(\"\\n=== [Keras TL Recipe] EfficientNetB0 Freeze ===\")\n",
        "print(\"LR:\", 1e-3, \"Epochs:\", tl_ep1, \"Backbone trainable:\", False)\n",
        "\n",
        "results_2.append({\n",
        "    \"Example\": \"2-Vision\",\n",
        "    \"Method\": \"Transfer\",\n",
        "    \"Model\": f\"KerasApp EffNetB0 (Freeze, ep={tl_ep1}, lr=1e-3)\",\n",
        "    **m_tl1,\n",
        "    \"Train_Min\": round(t_freeze, 2)\n",
        "})\n",
        "\n",
        "# Strateji-2: Fine-tune (unfreeze) düşük LR\n",
        "tl_ep2 = max(1, min(3, MAX_EPOCHS-1))\n",
        "t0 = time.time()\n",
        "model2 = build_keras_app_model(trainable_backbone=True)\n",
        "model2.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-5),\n",
        "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "    metrics=[\"accuracy\"]\n",
        ")\n",
        "model2.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=tl_ep2, batch_size=32, verbose=1)\n",
        "t_ft = (time.time() - t0) / 60.0\n",
        "\n",
        "m_tl2 = eval_keras(model2, X_test, y_test)\n",
        "\n",
        "print(\"\\n=== [Keras TL Recipe] EfficientNetB0 FineTune ===\")\n",
        "print(\"LR:\", 1e-5, \"Epochs:\", tl_ep2, \"Backbone trainable:\", True)\n",
        "\n",
        "results_2.append({\n",
        "    \"Example\": \"2-Vision\",\n",
        "    \"Method\": \"Transfer\",\n",
        "    \"Model\": f\"KerasApp EffNetB0 (FineTune, ep={tl_ep2}, lr=1e-5)\",\n",
        "    **m_tl2,\n",
        "    \"Train_Min\": round(t_ft, 2)\n",
        "})\n",
        "\n",
        "df_res2 = pd.DataFrame(results_2)\n",
        "display(df_res2)\n",
        "\n",
        "# -----------------------------\n",
        "# 6) Ayrıntılı rapor notu (hata ve çözüm)\n",
        "# -----------------------------\n",
        "print(\"\\n=== [RAPOR NOTU] Hata ve Çözüm ===\")\n",
        "print(\"Hata: ValueError: setting an array element with a sequence (inhomogeneous shape)\")\n",
        "print(\"Neden: tf_flowers görüntüleri farklı boyutlarda geldiği için np.array(images) 4D tensör oluşturamadı.\")\n",
        "print(\"Çözüm: tf.data pipeline üzerinde preprocess() ile tüm görüntüler (IMG_SIZE, IMG_SIZE) boyutuna resize edildi.\")\n",
        "print(\"Sonuç: AutoKeras ve Keras Applications eğitimleri tek tip tensörle sorunsuz çalışır.\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}